# KNN k-近邻算法
*****

*****
## KNN 概述
k-近邻（KNN，K-NearestNeighbor）算法是一种基本分类与回归方法，现在这里只讨论分类问题中的k-近邻算法  

KNN算法用一句话总结就是：近朱者赤近墨者黑  

k-近邻算法的输入为实例的特征向量，对应于特征空间的点，输出为实例的类别，可以取多个类别，即多分类。k近邻算法假设给定一个训练数据集，其中的实例类别已定。分类时对新的实例根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k-近邻算法不具有显式的学习过程。

k-近邻算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。k值得选择、距离度量以及分类决策规则是k-近邻算法的三个基本要素。

## KNN 场景
这里用一个十分老套的例子进行说明：  
电影可以按照题材分类，如何让机器区分**动作片** 和 **爱情片** 呢？
1. 动作片：打斗次数多
2. 爱情片：亲吻次数多
基于电影中的亲吻、打斗出现的次数，使用k-近邻算法构造程序，就可以自动划分电影的题材类型

|电影名称|打斗次数|亲吻次数|电影题材|
|---|----|-----|-----|
|A|3|104|爱情片|
|B|2|100|爱情片|
|C|1|81|爱情片|
|D|101|10|动作片|
|E|99|5|动作片|
|F|98|2|动作片|
|G|32|85|?|

#### 计算未知电影到已知电影之间的距离
我们将上述的电影按作打斗次数和亲吻次数在直角坐标系中标出，如果G电影是爱情电影，那么他在坐标系上的点更接近爱情片所在的范围，如果G电影是动作片，他在坐标系上的点更接近爱情片所在的范围，因此，我们计算所有电影和未知电影的距离，按照距离递增排序，结果如下  

|电影名称|与未知电影的位置|
|-------|---------------|
|C|31.26|
|B|33.54|
|A|34.67|
|D|101.91|
|E|104.35|
|F|106.04|

当我们设定 K = 3 是，前三个最近的点 分别为 C,B,A ,三个点有3个爱情片，0个动作片，所以我们可以认为 G 电影可能为 爱情片

## KNN 原理
### 工作原理
1. 假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据和所属分类的对应关系
2. 输入没有标签的新数据，将新数据的每个特征与样本集中数据对应的特征进行比较
  i. 计算新数据和样本数据集中每条数据的距离
  ii. 对求得的所有距离进行递增排序
  iii. 取前k个样本数据对应的分类标签
3. 将 k 个数据中出现次数最多的标签作为新数据的分类
### 通俗理解
给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。
### KNN 算法特点
**优点**：精度高、对异常值不敏感、无数据输入假定
**缺点**：计算复杂度高、空间复杂度高
**适用数据范围**：数值型和标称型

## KNN项目案例
### 项目一：优化约会对象的配对效果
#### 项目概述
渣女小倩使用约会网站寻找约会对象，经过一段时间之后，她归纳总结了曾经交往的三种类型的人
* 不喜欢的那屌丝
* 魅力一般的普通人
* 极具魅力的高富帅
她希望
1. 工作日与魅力一般的人普通人约会
2. 周末与极具魅力的高富帅约会
3. 不喜欢的男屌丝则直接排除掉

现在她收集了一些约会网站曾经记录的数据信息，希望通过这些数据对将来的约会对象进行归类

小倩把这些约会对象的数据存放在文本文件 datingTestSet2.txt 中，总共有 1000 行。小倩约会的对象主要包含以下3种特征：
* 每年获得的飞行常客里程数
* 玩视频游戏所消耗时间百分比
* 每周消耗的冰淇淋公升数
文本文件数据格式如下：
```
40920	8.326976	0.953952	3
14488	7.153469	1.673904	2
26052	1.441871	0.805124	1
75136	13.147394	0.428964	1
38344	1.669788	0.134296	1
```
### 项目二：手写数字识别系统
#### 代码概述
构造一个能识别数字0~9的基于KNN分类器的手写数字识别系统  
需要识别的数字 是存储在文本文件中的具有相同色彩和大小：宽高是 32 * 32 像素的黑白图像
## KNN 小结
KNN 是一个简单的无显示学习过程，它本身没有任何参数，更谈不上学习
KNN 是非泛化学习的监督学习模型。 在分类和回归中均有使用
### 基本原理
简单来说：通过距离量度来计算查询点（query point）与每个训练数据点的距离，然后选出与查询点（query point）相近的k个最近邻点（k nearest neighbors）,使用分类决策选出对应的标签来作为该查询点的标签
### KNN 三要素
#### 1.k 的取值
   对查询点标签有很大的影响。 k 值小的时候 近似误差小，估计误差大。k 值大的时候 近似误差大，估计误差小
   
   k 值取小了，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差（approximation error）会减小，只有与输入实例较近的（相似的）训练    实例才会对预测结果起作用。但缺点是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪     声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。
   
   k 值取大了，那么就可能忽略到训练集的一般规律，你可以想象k值取训练集的数据数目，那么无论测试数据怎么变，结果都是测试集中数目较多的标签。就     相当于用较大的邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练     实例也会对预测起作用，使预测发生错误。 k 值的增大就意味着整体的模型变得简单。
   
   **近似误差**：对现有训练集的训练误差
   **估计误差**：对测试集的测试误差
   [近似误差和估计误差区别](https://www.zhihu.com/question/60793482)
   
#### 2.距离量度
   欧式距离：以空间为基准的两点之间最短距离
   曼哈顿距离：
   ChebyShev 距离：
   明式距离：
   [四种距离量度的区别](https://blog.csdn.net/jerry81333/article/details/52632687)

### 算法:(参考 sklearn 库)
Brute Force 暴击计算/线性扫描
KD Tree 使用二叉树根据数据维度来平分参数空间
Ball Tree 使用一系列超球体来平分训练数据集

   
   
